{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3eREw2pcdqm"
      },
      "source": [
        "# Basic RAG System with LangChain\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPj5mhnc4lp_"
      },
      "source": [
        "### Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J_Z9mKEa8fM",
        "outputId": "3a610e4d-5d66-4582-9975-74ce3a8fbf21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain==0.3.10\n",
            "  Downloading langchain-0.3.10-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.22 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.3.24)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.3.2)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.3.10)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.10) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.2.2)\n",
            "Downloading langchain-0.3.10-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langsmith, langchain\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.2.2\n",
            "    Uninstalling langsmith-0.2.2:\n",
            "      Successfully uninstalled langsmith-0.2.2\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.11\n",
            "    Uninstalling langchain-0.3.11:\n",
            "      Successfully uninstalled langchain-0.3.11\n",
            "Successfully installed langchain-0.3.10 langsmith-0.1.147\n",
            "Collecting langchain-openai==0.2.12\n",
            "  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.2.12) (0.3.24)\n",
            "Collecting openai<2.0.0,>=1.55.3 (from langchain-openai==0.2.12)\n",
            "  Downloading openai-1.57.4-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai==0.2.12)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.1.147)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (4.66.6)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.2.3)\n",
            "Downloading langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.57.4-py3-none-any.whl (390 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.3/390.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.5\n",
            "    Uninstalling openai-1.54.5:\n",
            "      Successfully uninstalled openai-1.54.5\n",
            "Successfully installed langchain-openai-0.2.12 openai-1.57.4 tiktoken-0.8.0\n",
            "Collecting langchain-community==0.3.11\n",
            "  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (3.11.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.11)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community==0.3.11)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.11 (from langchain-community==0.3.11)\n",
            "  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (0.3.24)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.11)\n",
            "  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.11)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.11) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.27.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.2.2)\n",
            "Downloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain, langchain-community\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.10\n",
            "    Uninstalling langchain-0.3.10:\n",
            "      Successfully uninstalled langchain-0.3.10\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.11 langchain-community-0.3.11 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.7.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
            "Collecting jq\n",
            "  Downloading jq-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Downloading jq-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jq\n",
            "Successfully installed jq-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.3.10\n",
        "!pip install langchain-openai==0.2.12\n",
        "!pip install langchain-community==0.3.11\n",
        "!pip install jq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9SJBRTmdiF5",
        "outputId": "356e458a-0b9b-4546-8c39-60f1eeecd3a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-chroma==0.1.4\n",
            "  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma==0.1.4)\n",
            "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting fastapi<1,>=0.95.2 (from langchain-chroma==0.1.4)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1.40 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma==0.1.4) (0.3.24)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma==0.1.4) (1.26.4)\n",
            "Collecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.10.3)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading uvicorn-0.33.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.28.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.28.2)\n",
            "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.20.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.66.6)\n",
            "Collecting overrides>=7.3.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.68.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.15.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.10.12)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (13.9.4)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi<1,>=0.95.2->langchain-chroma==0.1.4)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (0.1.147)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (3.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.2.3)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (1.0.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.17.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.27.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.26.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (14.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.4.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.6.1)\n",
            "Downloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n",
            "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.33.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=1a3c0b487c89795476bf3cc9d62b32071f6537324222aa9aff2b3281eda2d1e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb, langchain-chroma\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.28.2\n",
            "    Uninstalling opentelemetry-api-1.28.2:\n",
            "      Successfully uninstalled opentelemetry-api-1.28.2\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.49b2\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.49b2:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.49b2\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.28.2\n",
            "    Uninstalling opentelemetry-sdk-1.28.2:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.28.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.6 httptools-0.6.4 humanfriendly-10.0 kubernetes-31.0.0 langchain-chroma-0.1.4 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.7.4 protobuf-5.29.1 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.41.3 uvicorn-0.33.0 uvloop-0.21.0 watchfiles-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-chroma==0.1.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtBa7rlWJWH3"
      },
      "source": [
        "## Enter API Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av1UpSgXZUsI",
        "outputId": "878bb0a1-cbe2-4af2-daa6-1e3154ee0d86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI Key: ··········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter your OpenAI Key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5rOqCyianbP"
      },
      "source": [
        "## Setup Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PIStD04Zp9p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmUCqWtf5gpk"
      },
      "source": [
        "### Load Wikipedia Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQjM-gKUDT1Q",
        "outputId": "622555fb-ccf6-4611-f1fa-1dfbb620eede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aZxZejfteVuofISodUrY2CDoyuPLYDGZ\n",
            "To: /content/rag_docs.zip\n",
            "\r  0% 0.00/5.92M [00:00<?, ?B/s]\r 62% 3.67M/5.92M [00:00<00:00, 36.3MB/s]\r100% 5.92M/5.92M [00:00<00:00, 45.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1aZxZejfteVuofISodUrY2CDoyuPLYDGZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qoTMZ1tPJAg",
        "outputId": "a873f9e5-5f93-49e8-a519-f17438f61f62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  rag_docs.zip\n",
            "   creating: rag_docs/\n",
            "  inflating: rag_docs/attention_paper.pdf  \n",
            "  inflating: rag_docs/cnn_paper.pdf  \n",
            "  inflating: rag_docs/resnet_paper.pdf  \n",
            "  inflating: rag_docs/vision_transformer.pdf  \n",
            "  inflating: rag_docs/wikidata_rag_demo.jsonl  \n"
          ]
        }
      ],
      "source": [
        "!unzip rag_docs.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_NrBVzteHaZ"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import JSONLoader\n",
        "\n",
        "loader = JSONLoader(file_path='./rag_docs/wikidata_rag_demo.jsonl',\n",
        "                    jq_schema='.',\n",
        "                    text_content=False,\n",
        "                    json_lines=True)\n",
        "passages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e13_oFtlDT1S",
        "outputId": "b72594d5-45d3-4c64-f38d-3dbba9ac9785"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1801"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(passages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm77itlPEDx0",
        "outputId": "36e70a19-0af9-4e48-9754-72d93d56f5c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/rag_docs/wikidata_rag_demo.jsonl', 'seq_num': 4}, page_content='{\"id\": \"71548\", \"title\": \"Chi-square distribution\", \"paragraphs\": [\"In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1\\\\u00a0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution.\", \"Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.\"]}')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "passages[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "PIXAO88utPDi",
        "outputId": "e283e4e3-c382-4046-98db-fb62a9307da7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n",
              "\n",
              "Example:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.documents import Document\n",
              "\n",
              "        document = Document(\n",
              "            page_content=&quot;Hello, world!&quot;,\n",
              "            metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n",
              "        )</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 262);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "langchain_core.documents.base.Document"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(passages[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZUxQwCl5jY-"
      },
      "source": [
        "### Load Open AI LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NKUgjxLL-Ux"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1nD_kYA5ncq"
      },
      "source": [
        "### Generate LLM Embeddings and store them in Chroma Vector DB\n",
        "\n",
        "**Chroma Vector DB** is a versatile, open-source vector database designed for managing and querying vector embeddings. It is easy to set up and integrates well with various AI tools and algorithms. Chroma is particularly useful for applications that require rapid and precise retrieval of content represented as embeddings—efficient data formats for text, images, and soon, audio and video.\n",
        "\n",
        "**Key Features:**\n",
        "- **Integration with AI Tools:** Chroma supports embedding functions from leading providers like OpenAI, Google, and Hugging Face, allowing for flexible and powerful data handling.\n",
        "- **Ease of Use:** The database provides default embedding functions, or users can integrate external APIs to generate embeddings.\n",
        "- **Efficient Querying:** Users can create collections to store embeddings, documents, and metadata. These can be queried to retrieve the most similar items, making information retrieval quick and effective.\n",
        "- **Flexible API:** Chroma offers a straightforward API that supports both standard operations and custom embedding functions.\n",
        "\n",
        "For more detailed information, visit the official Chroma documentation [here](https://docs.trychroma.com).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbEe-0csNRmB",
        "outputId": "43acef75-6b23-4446-a830-46215e6158af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/rag_docs/wikidata_rag_demo.jsonl', 'seq_num': 1}, page_content='{\"id\": \"84801\", \"title\": \"Chinese New Year\", \"paragraphs\": [\"Chinese New Year, known in China as the SpringFestival and in Singapore as the LunarNewYear, is a holiday on and around the new moon on the first day of the year in the traditional Chinese calendar. This calendar is based on the changes in the moon and is only sometimes changed to fit the seasons of the year based on how the Earth moves around the sun. Because of this, Chinese New Year is never on January1. It moves around between January21 and February20.\", \"The Chinese New Year is of the most important holidays for Chinese people all over the world. Its 7th day used to be used instead of birthdays to count people\\'s ages in China. The holiday is still used to tell people which \\\\\"animal\\\\\" of the Chinese zodiac they are part of. The holiday is a time for gifts to children and for family gatherings with large meals, just like Christmas in Europe and in other Christian areas. Unlike Christmas, the children usually get gifts of cash in red envelopes (\\\\\"hongbao\\\\\") and not toys or clothes.\", \"Chinese New Year used to last 15 days until the Lantern Festival on the year\\'s first full moon. Now, it is a national holiday in the Republic and People\\'s Republic of China, the Philippines, Singapore, Malaysia, Brunei, and Indonesia. It is also celebrated in some parts of Thailand. In some places, only the first day or three days are celebrated. In the PRC, nearby weekends are changed to create a 7-day-long \\\\\"Golden Week\\\\\".\"]}'),\n",
              " Document(metadata={'source': '/content/rag_docs/wikidata_rag_demo.jsonl', 'seq_num': 2}, page_content='{\"id\": \"86394\", \"title\": \"Dattatreya\", \"paragraphs\": [\"Dattatreya is the God who is an incarnation of the Divine Trinity Brahma, Vishnu and Siva. The word Datta means \\\\\"Given\\\\\", Datta is called so because the divine trinity have \\\\\"given\\\\\" themselves in the form of a son to the sage couple Guru Atri and Mata Anusuya. He is the son of Guru Atri, hence the name \\\\\"Atreya.\\\\\"\", \"In the Nath tradition, Dattatreya is seen as an Avatar or incarnation of the Lord Shiva and as the Adi-Guru (First Teacher) of the Adi-Nath sampradaya of the Nathas. Although Dattatreya was at first a \\\\\"Lord of Yoga\\\\\" with Tantric traits, he was adapted and assimilated into the more devotional cults; while still worshiped by millions of Hindus, he is approached more as a benevolent God than as a teacher of the highest essence of Indian thought.\", \"Though the Dattatreya of the Natha tradition coexisted and intermingled with the Puranic, Brahmanical tradition of the Datta sampradaya, here we shall focus almost exclusively on the earlier Tantric manifestation of Datta. Shri Gurudev Mahendranath had no doubt that Dattatreya was an historical figure. He stated that Datta was born on Wednesday, the fourteenth day of the full moon in the month of Margasirsa, though he does not mention the year.\"]}'),\n",
              " Document(metadata={'source': '/content/rag_docs/wikidata_rag_demo.jsonl', 'seq_num': 3}, page_content='{\"id\": \"87272\", \"title\": \"Thelema\", \"paragraphs\": [\"Thelema is the English spelling of the Greek noun : \\\\\"will\\\\\", from the verb \\\\\"\\\\u03b8\\\\u03ad\\\\u03bb\\\\u03c9\\\\\": to will, wish, purpose. Early Christian writings use the word to refer to the will of God, the human will, and even the will of God\\'s opponent, the Devil.\", \"Thelema is also a way of life first written about by Fran\\\\u00e7ois Rabelais (16th century) in his famous books of fiction, Gargantua and Pantagruel. The core of this way of life was summed up in the phrase \\\\\"\\\\\"Do what thou wilt\\\\\"\\\\\" (\\\\\"fay \\\\u00e7e que vouldras\\\\\" in the original old French). This idea was later put into practice in the mid 18th century by Sir Francis Dashwood at Medmenham.\", \"This Thelemic Law of Rabelais was revived by Aleister Crowley in 1904 when Crowley wrote \\\\\"The Book of the Law\\\\\". This book contains both the word \\\\\"Thelema\\\\\" in Greek as well as the phrase \\\\\"Do what thou wilt.\\\\\" From this, Crowley took Thelema as the name of his own religion. Thus Shri Gurudev Mahendranath wrote that Rabelais, Dashwood, and Crowley must share the honor of perpetuating Thelema.\"]}')]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "passages[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzr79saZ7dZM"
      },
      "source": [
        "{\"id\": \"87272\", \"title\": \"Thelema\", \"paragraphs\": [\"Thelema is the English spelling of the Greek noun : \\\\\"will\\\\\", from the verb \\\\\"\\\\u03b8\\\\u03ad\\\\u03bb\\\\u03c9\\\\\": to will, wish, purpose. Early Christian writings use the word to refer to the will of God, the human will, and even the will of God\\'s opponent, the Devil.\", \"Thelema is also a way of life first written about by Fran\\\\u00e7ois Rabelais (16th century) in his famous books of fiction, Gargantua and Pantagruel. The core of this way of life was summed up in the phrase \\\\\"\\\\\"Do what thou wilt\\\\\"\\\\\" (\\\\\"fay \\\\u00e7e que vouldras\\\\\" in the original old French). This idea was later put into practice in the mid 18th century by Sir Francis Dashwood at Medmenham.\", \"This Thelemic Law of Rabelais was revived by Aleister Crowley in 1904 when Crowley wrote \\\\\"The Book of the Law\\\\\". This book contains both the word \\\\\"Thelema\\\\\" in Greek as well as the phrase \\\\\"Do what thou wilt.\\\\\" From this, Crowley took Thelema as the name of his own religion. Thus Shri Gurudev Mahendranath wrote that Rabelais, Dashwood, and Crowley must share the honor of perpetuating Thelema.\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzWipkvEvKrp",
        "outputId": "fd18344b-24b0-48a7-e913-16c40b4168ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1801/1801 [00:00<00:00, 34934.91it/s]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from langchain.docstore.document import Document\n",
        "from tqdm import tqdm\n",
        "\n",
        "processed_docs = []\n",
        "\n",
        "for doc in tqdm(passages):\n",
        "    processed_doc = json.loads(doc.page_content)\n",
        "    metadata = {\n",
        "        \"title\" : processed_do      c['title'],\n",
        "        \"id\" : processed_doc['id']\n",
        "    }\n",
        "    document_text = ' '.join(processed_doc['paragraphs'])\n",
        "    processed_docs.append(Document(page_content=document_text,\n",
        "                                   metadata=metadata))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XInwft89vqZx",
        "outputId": "21a2e9b0-34d2-42f1-8276-43d3df59513a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'title': 'Chinese New Year', 'id': '84801'}, page_content='Chinese New Year, known in China as the SpringFestival and in Singapore as the LunarNewYear, is a holiday on and around the new moon on the first day of the year in the traditional Chinese calendar. This calendar is based on the changes in the moon and is only sometimes changed to fit the seasons of the year based on how the Earth moves around the sun. Because of this, Chinese New Year is never on January1. It moves around between January21 and February20. The Chinese New Year is of the most important holidays for Chinese people all over the world. Its 7th day used to be used instead of birthdays to count people\\'s ages in China. The holiday is still used to tell people which \"animal\" of the Chinese zodiac they are part of. The holiday is a time for gifts to children and for family gatherings with large meals, just like Christmas in Europe and in other Christian areas. Unlike Christmas, the children usually get gifts of cash in red envelopes (\"hongbao\") and not toys or clothes. Chinese New Year used to last 15 days until the Lantern Festival on the year\\'s first full moon. Now, it is a national holiday in the Republic and People\\'s Republic of China, the Philippines, Singapore, Malaysia, Brunei, and Indonesia. It is also celebrated in some parts of Thailand. In some places, only the first day or three days are celebrated. In the PRC, nearby weekends are changed to create a 7-day-long \"Golden Week\".'),\n",
              " Document(metadata={'title': 'Dattatreya', 'id': '86394'}, page_content='Dattatreya is the God who is an incarnation of the Divine Trinity Brahma, Vishnu and Siva. The word Datta means \"Given\", Datta is called so because the divine trinity have \"given\" themselves in the form of a son to the sage couple Guru Atri and Mata Anusuya. He is the son of Guru Atri, hence the name \"Atreya.\" In the Nath tradition, Dattatreya is seen as an Avatar or incarnation of the Lord Shiva and as the Adi-Guru (First Teacher) of the Adi-Nath sampradaya of the Nathas. Although Dattatreya was at first a \"Lord of Yoga\" with Tantric traits, he was adapted and assimilated into the more devotional cults; while still worshiped by millions of Hindus, he is approached more as a benevolent God than as a teacher of the highest essence of Indian thought. Though the Dattatreya of the Natha tradition coexisted and intermingled with the Puranic, Brahmanical tradition of the Datta sampradaya, here we shall focus almost exclusively on the earlier Tantric manifestation of Datta. Shri Gurudev Mahendranath had no doubt that Dattatreya was an historical figure. He stated that Datta was born on Wednesday, the fourteenth day of the full moon in the month of Margasirsa, though he does not mention the year.'),\n",
              " Document(metadata={'title': 'Thelema', 'id': '87272'}, page_content='Thelema is the English spelling of the Greek noun : \"will\", from the verb \"θέλω\": to will, wish, purpose. Early Christian writings use the word to refer to the will of God, the human will, and even the will of God\\'s opponent, the Devil. Thelema is also a way of life first written about by François Rabelais (16th century) in his famous books of fiction, Gargantua and Pantagruel. The core of this way of life was summed up in the phrase \"\"Do what thou wilt\"\" (\"fay çe que vouldras\" in the original old French). This idea was later put into practice in the mid 18th century by Sir Francis Dashwood at Medmenham. This Thelemic Law of Rabelais was revived by Aleister Crowley in 1904 when Crowley wrote \"The Book of the Law\". This book contains both the word \"Thelema\" in Greek as well as the phrase \"Do what thou wilt.\" From this, Crowley took Thelema as the name of his own religion. Thus Shri Gurudev Mahendranath wrote that Rabelais, Dashwood, and Crowley must share the honor of perpetuating Thelema.')]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_docs[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyFSs6xZfzok"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPRVzeE6NwE7"
      },
      "outputs": [],
      "source": [
        "# The vectorstore we'll be using\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# The splitting and chunking strategy\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_9jsBAJWMk5"
      },
      "outputs": [],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=3000,\n",
        "                                          chunk_overlap=200)\n",
        "chunked_docs = splitter.split_documents(processed_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tu4rqnWYLFI",
        "outputId": "1af8640e-c421-493a-f264-9537c5b82bd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'title': 'Chinese New Year', 'id': '84801'}, page_content='Chinese New Year, known in China as the SpringFestival and in Singapore as the LunarNewYear, is a holiday on and around the new moon on the first day of the year in the traditional Chinese calendar. This calendar is based on the changes in the moon and is only sometimes changed to fit the seasons of the year based on how the Earth moves around the sun. Because of this, Chinese New Year is never on January1. It moves around between January21 and February20. The Chinese New Year is of the most important holidays for Chinese people all over the world. Its 7th day used to be used instead of birthdays to count people\\'s ages in China. The holiday is still used to tell people which \"animal\" of the Chinese zodiac they are part of. The holiday is a time for gifts to children and for family gatherings with large meals, just like Christmas in Europe and in other Christian areas. Unlike Christmas, the children usually get gifts of cash in red envelopes (\"hongbao\") and not toys or clothes. Chinese New Year used to last 15 days until the Lantern Festival on the year\\'s first full moon. Now, it is a national holiday in the Republic and People\\'s Republic of China, the Philippines, Singapore, Malaysia, Brunei, and Indonesia. It is also celebrated in some parts of Thailand. In some places, only the first day or three days are celebrated. In the PRC, nearby weekends are changed to create a 7-day-long \"Golden Week\".'),\n",
              " Document(metadata={'title': 'Dattatreya', 'id': '86394'}, page_content='Dattatreya is the God who is an incarnation of the Divine Trinity Brahma, Vishnu and Siva. The word Datta means \"Given\", Datta is called so because the divine trinity have \"given\" themselves in the form of a son to the sage couple Guru Atri and Mata Anusuya. He is the son of Guru Atri, hence the name \"Atreya.\" In the Nath tradition, Dattatreya is seen as an Avatar or incarnation of the Lord Shiva and as the Adi-Guru (First Teacher) of the Adi-Nath sampradaya of the Nathas. Although Dattatreya was at first a \"Lord of Yoga\" with Tantric traits, he was adapted and assimilated into the more devotional cults; while still worshiped by millions of Hindus, he is approached more as a benevolent God than as a teacher of the highest essence of Indian thought. Though the Dattatreya of the Natha tradition coexisted and intermingled with the Puranic, Brahmanical tradition of the Datta sampradaya, here we shall focus almost exclusively on the earlier Tantric manifestation of Datta. Shri Gurudev Mahendranath had no doubt that Dattatreya was an historical figure. He stated that Datta was born on Wednesday, the fourteenth day of the full moon in the month of Margasirsa, though he does not mention the year.'),\n",
              " Document(metadata={'title': 'Thelema', 'id': '87272'}, page_content='Thelema is the English spelling of the Greek noun : \"will\", from the verb \"θέλω\": to will, wish, purpose. Early Christian writings use the word to refer to the will of God, the human will, and even the will of God\\'s opponent, the Devil. Thelema is also a way of life first written about by François Rabelais (16th century) in his famous books of fiction, Gargantua and Pantagruel. The core of this way of life was summed up in the phrase \"\"Do what thou wilt\"\" (\"fay çe que vouldras\" in the original old French). This idea was later put into practice in the mid 18th century by Sir Francis Dashwood at Medmenham. This Thelemic Law of Rabelais was revived by Aleister Crowley in 1904 when Crowley wrote \"The Book of the Law\". This book contains both the word \"Thelema\" in Greek as well as the phrase \"Do what thou wilt.\" From this, Crowley took Thelema as the name of his own religion. Thus Shri Gurudev Mahendranath wrote that Rabelais, Dashwood, and Crowley must share the honor of perpetuating Thelema.')]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunked_docs[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RXepcnd1E7b"
      },
      "source": [
        "## Create Vector DB and Retriever\n",
        "\n",
        "If you have already created `wiki_db`in the previous hands-on session then just load the DB and DO NOT run the following code to create the database again, ignore this when running on Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_st2XDc3N2Kf"
      },
      "outputs": [],
      "source": [
        "# create vector DB of docs and embeddings - takes 1 min on Colab\n",
        "chroma_db = Chroma.from_documents(documents=chunked_docs,\n",
        "                                  collection_name='wiki_db',\n",
        "                                  embedding=openai_embed_model,\n",
        "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
        "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
        "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                  persist_directory=\"./wiki_db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ju_zBIj1Zsb"
      },
      "source": [
        "## Load Vector DB from disk\n",
        "\n",
        "Run the following code if your vector DB already exists on disk from the previous hands-on session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNvj0dDH1WDg"
      },
      "outputs": [],
      "source": [
        "# load from disk\n",
        "chroma_db = Chroma(persist_directory=\"./wiki_db\",\n",
        "                   collection_name='wiki_db',\n",
        "                   embedding_function=openai_embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFC3uPqYop0a",
        "outputId": "aa4775ed-4e79-4757-8306-6e5ef4fb9635"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain_chroma.vectorstores.Chroma at 0x7a914af45ed0>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chroma_db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTmvHpCRlXOO"
      },
      "outputs": [],
      "source": [
        "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                              search_kwargs={\"k\": 5, \"score_threshold\": 0.2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCEckuS4zzMl",
        "outputId": "355060d4-f837-4629-8cf6-15ec14387995"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'id': '564928', 'title': 'Machine learning'}, page_content='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'),\n",
              " Document(metadata={'id': '359370', 'title': 'Supervised learning'}, page_content='In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data.'),\n",
              " Document(metadata={'id': '663523', 'title': 'Deep learning'}, page_content='Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences.'),\n",
              " Document(metadata={'id': '6360', 'title': 'Artificial intelligence'}, page_content='Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology.'),\n",
              " Document(metadata={'id': '44742', 'title': 'Artificial neural network'}, page_content='A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation.')]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "similarity_retriever.invoke('what is machine learning?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaLrMvJ-z-6_",
        "outputId": "cc372f73-d6d0-4302-8a6f-9ffedbbf7eff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'id': '6360', 'title': 'Artificial intelligence'}, page_content='Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology.'),\n",
              " Document(metadata={'id': '674015', 'title': 'A.I. Artificial Intelligence'}, page_content='A.I. Artificial Intelligence, or A.I., is a 2001 American science fiction drama movie directed by Steven Spielberg. The screenplay was by Spielberg based on the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss. The movie was produced by Kathleen Kennedy, Spielberg and Bonnie Curtis. It stars Haley Joel Osment, Jude Law, Frances O\\'Connor, Brendan Gleeson and William Hurt. It is set in a futuristic post-climate change society. \"A.I.\" tells the story of David (Osment), a childlike android uniquely programmed with the ability to love.'),\n",
              " Document(metadata={'id': '416249', 'title': 'Artificial Intelligence (album)'}, page_content='Artificial Intelligence is the tenth studio album by Welsh multi-instrumentalist John Cale. It was released in September 1985. It was his only full-length album on Beggars Banquet Records label. It was produced by Cale himself.'),\n",
              " Document(metadata={'id': '669662', 'title': 'Loop AI Labs'}, page_content='Loop AI Labs is an AI and cognitive computing company that focuses on language understanding technology. The company was founded in San Francisco in 2012 by Italian entrepreneur Gianmauro Calafiore, who sold his company Gsmbox to in 2004 and then relocated from Italy to San Francisco. Wanting to start an artificial intelligence company, he recruited two veterans of the project, the largest government-funded AI project in history, who had worked on the project at and Stanford University\\'s . The original company name, \"Soshoma\", was changed to Loop AI Labs in 2015 after the company decided to change its focus from consumer-oriented to enterprise. Loop AI Labs is headquartered in San Francisco, California, with offices in New York, Milan, and Singapore. The company is privately funded. On May 4, 2017, Loop AI Labs entered into a deal with , a leading European provider of mobile messaging and solutions, to bring their cognitive computing technology to LINK\\'s business clients, which cover 234 million people across Europe.'),\n",
              " Document(metadata={'id': '564218', 'title': 'Robot lawyer'}, page_content='A robot lawyer is an artificial intelligence (AI) computer program. It is designed to ask the same questions as a real lawyer about certain legal issues. Robot lawyers are being used in many countries around the world including the United States, the United Kingdom, and Holland.')]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "similarity_retriever.invoke('what is ai?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV0RMcHQ0GrY",
        "outputId": "981758c4-00d8-4e58-9241-a1385fcf5dfa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'id': '639943', 'title': '2017–18 UEFA Europa League'}, page_content=\"The 2017–18 UEFA Europa League was the 47th season of Europe's secondary club football tournament organised by UEFA, and the 9th season since it was renamed from the UEFA Cup to the UEFA Europa League. The last game was played at the Parc Olympique Lyonnais in Décines-Charpieu, France. Atlético Madrid won against Marseille to win their third Europa League title. As winners, Atlético Madrid played against the winners of the 2017–18 UEFA Champions League, Real Madrid, in the 2018 UEFA Super Cup. They also automatically qualified for the 2018–19 UEFA Champions League group stage, but since they had already qualified through their league performance, the berth reserved was given to the third-placed team of the 2017–18 Ligue 1.\"),\n",
              " Document(metadata={'id': '3070', 'title': 'Chelsea F.C.'}, page_content=\"Chelsea F.C. is an English football club that plays in the English Premier League. Their home stadium is Stamford Bridge in Fulham, London. Chelsea is considered to be one of the most successful clubs of England, having won many trophies, including 6 Premier League, 1 UEFA Champions League, 2 UEFA Europa League, 5 League Cup, 8 FA Cup, 1 UEFA Super Cup and 2 UEFA Cup Winners' Cup titles. Their all-time highest goalscorer is Frank Lampard and their most successful goalkeeper (on the basis of clean sheets and titles statistics) is Petr Čech. Chelsea is owned by the Russian billionaire businessman Roman Abramovich. Chelsea started in 1905 and played the second division of the league. They won their first trophy in 1955, when they became Champions of the First Division. They won the FA Cup in 1970, 1997, 2000 and 2007. They won the League Cup in 1965, 1998, 2005 and 2007. In 1970s Chelsea failed to maintain their position of the first division, due to the financial difficulties. In 1990s they challenged the title of Premier League. They came close but did not win it until 2005 and 2006.\"),\n",
              " Document(metadata={'id': '52593', 'title': 'Juventus F.C.'}, page_content='Juventus F.C., sometimes known as Juve, is an Italian football club that plays in Serie A. It was founded in 1897 and they play their home games at the Juventus Stadium in Turin. The club is the most successful team in the history of Italian football. Overall, the club has won 51 official trophies, more than any other team in the country; 40 in Italy, which is also a record, and 11 in European and world competitions. The \"Old Lady\" is the third most successful club in Europe and the sixth in the world with the most international titles officially recognized by one of the six continental football confederations and FIFA. The club was the first Italian to win the UEFA Cup. In 1985, Juventus, the only team in the world to have won all official international cups and championships became the first club in the history of European football to have won all three major UEFA club competitions.'),\n",
              " Document(metadata={'id': '611374', 'title': 'MLS Cup 2016'}, page_content='MLS Cup 2016 was a soccer game played on December 10, 2016. It was the 21st MLS Cup, a special game between the winners of the Eastern and Western Conferences in Major League Soccer. Seattle Sounders FC won the game in a penalty shootout with a score of 5–4 against Toronto FC.. The two teams played again in MLS Cup 2017. Toronto FC won that game 2–0.'),\n",
              " Document(metadata={'id': '54082', 'title': 'Scottish Premier League'}, page_content=\"The Scottish Premier League (known as the Clydesdale Bank Premier League or SPL) was a professional competition for football clubs. It was the top level of the Scottish football league system. In 2013 the Scottish Premier league was replaced by the Scottish Premiership as Scotland's top football competition. In Scotland, watching the SPL is more popular than watching the top football leagues in any other European country. The Scottish Premier League is currently twentieth in the UEFA rankings of European leagues, which are based on the how well the participating clubs play in all European competitions. Eighteen clubs have played in the SPL since it began in 1998, but only two have won the title: the Old Firm of Celtic and Rangers both seven times. The SPL began in 1998, when a group of teams decided to split from the Scottish Football League and create their own league. The idea of splitting from the league was from a similar event in 1992 with the start of the English Premiership.\")]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "similarity_retriever.invoke('who won the champions league?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfn4Z4Em55bb"
      },
      "source": [
        "### Build a QA RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Pu5U9HNkl-q"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "            Use the following pieces of retrieved context to answer the question.\n",
        "            If the answer is not present in the context, just say that you don't know.\n",
        "            Keep the answer to the point.\n",
        "\n",
        "            Question:\n",
        "            {question}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            Answer:\n",
        "         \"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MpA4HCDw_fP",
        "outputId": "0d12e17c-26d8-46a2-e4f5-1145bde19c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "You are an assistant for question-answering tasks.\n",
            "            Use the following pieces of retrieved context to answer the question.\n",
            "            If the answer is not present in the context, just say that you don't know.\n",
            "            Keep the answer to the point.\n",
            "\n",
            "            Question:\n",
            "            \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n",
            "\n",
            "            Context:\n",
            "            \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
            "\n",
            "            Answer:\n",
            "         \n"
          ]
        }
      ],
      "source": [
        "prompt_template.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng4cKPYfmRyg"
      },
      "source": [
        "## RAG Chain - Using LCEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18p2bJahmLX_"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "qa_rag_chain = (\n",
        "    {\n",
        "        \"context\": (similarity_retriever\n",
        "                      |\n",
        "                    format_docs),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "       |\n",
        "    prompt_template\n",
        "      |\n",
        "    chatgpt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIbM9cXn-2WO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "njFlSt9Smznk",
        "outputId": "89205692-6f6d-4be8-a8c5-20746b662fb6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Machine learning is a subfield of computer science that gives computers the ability to learn from data without being explicitly programmed. It involves the study and construction of algorithms that can learn and make predictions or decisions based on data, building models from sample inputs."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "query = \"What is machine learning?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "PS5f9QKVn935",
        "outputId": "8f49c464-fd17-446d-9a96-ec7dc3dc1a73"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I don't know."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"Who was the winner of the champions league in 2020?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "-OfsGdmhnn6u",
        "outputId": "9e7747d7-0678-4eea-c8c4-dccf3c712f00"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn, and it is a field of study aimed at making computers \"smart.\" AI systems can interpret external data, learn from it, and use those learnings to achieve specific goals through flexible adaptation."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"What is AI?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "VwnV1fvWPILC",
        "outputId": "6dd550d1-69d2-475e-a687-42fa41f6ab7c"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I don't know."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"How is it different from statistics\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVemECCPfYg0"
      },
      "source": [
        "# Conversational RAG System with LangChain\n",
        "\n",
        "In many Q&A applications, the ability to engage in back-and-forth conversations with users is crucial. This necessitates the application having a form of \"memory\" to recall past interactions and apply this context to current queries.\n",
        "\n",
        "This guide focuses on integrating historical messages into the application's logic. Additional details on managing chat history can be found [here](https://python.langchain.com/docs/expression_language/how_to/message_history/).\n",
        "\n",
        "![](https://i.imgur.com/8hLJMPl.gif)\n",
        "\n",
        "### Building on the Q&A RAG System - to a Conversational Q&A RAG System\n",
        "\n",
        "We will enhance our Q&A RAG System, which utilizes the Wikipedia dataset, by implementing the following updates:\n",
        "\n",
        "- **Prompt Adjustment:** Our prompt will be modified to include historical messages as inputs, allowing the system to maintain context over the course of a conversation.\n",
        "\n",
        "- **Contextualizing Questions:** We will introduce a sub-chain mechanism to reformulate the latest user query by considering the chat history. This is crucial for understanding questions that refer back to previous messages. For example, a query like \"Can you elaborate on the second point?\" relies on the context provided by preceding interactions, which affects the system's ability to retrieve relevant information effectively.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0sMxGqEojYM"
      },
      "source": [
        "## Contextualizing the Question\n",
        "\n",
        "To maintain a seamless flow in conversations, especially in a Q&A setting, it's essential to incorporate historical interactions. Here’s how we achieve this:\n",
        "\n",
        "### Defining a Sub-Chain for Historical Context\n",
        "\n",
        "1. **Sub-Chain Creation:** We'll define a sub-chain that uses both historical messages and the latest user query. This sub-chain reformulates the question if it refers to any past interactions, ensuring the system, especially the vector database understands the context to return the most relevant documents to this newly reworded question.\n",
        "\n",
        "2. **Using `MessagesPlaceholder`:** Our prompt construction involves a `MessagesPlaceholder` variable named `chat_history`. This setup allows us to input a list of messages using the `chat_history` key. The system integrates these messages, positioning them after its own responses and before the latest user question.\n",
        "\n",
        "3. **Helper Function Usage:** We employ the `create_history_aware_retriever` function available [here](https://api.python.langchain.com/en/latest/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html). This function is crucial for handling instances where the chat history might be empty and orchestrates the sequence of operations: `prompt | llm | StrOutputParser() | retriever`.\n",
        "\n",
        "4. **Chain Construction:** The `create_history_aware_retriever` constructs a chain that processes inputs under the keys `input` and `chat_history`, ensuring the output schema aligns with that of a retriever.\n",
        "\n",
        "By implementing these steps, our system can effectively utilize historical context to better understand and respond to user queries, thereby enhancing the conversational experience.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Drh0P1d2s0m"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "rephrase_system_prompt = \"\"\"Given a chat history and the latest user question\n",
        "which might reference context in the chat history, formulate a standalone question\n",
        "which can be understood without the chat history. Do NOT answer the question,\n",
        "just reformulate it if needed and otherwise return it as is.\n",
        "\"\"\"\n",
        "\n",
        "rephrase_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", rephrase_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    chatgpt, similarity_retriever, rephrase_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CjQ0iQU9vzk",
        "outputId": "cf1d2d0c-f2e4-4e29-e020-08734a26a6b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
              "| VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7a914af45ed0>, search_type='similarity_score_threshold', search_kwargs={'k': 5, 'score_threshold': 0.2}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7a9165aee290>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Given a chat history and the latest user question\\nwhich might reference context in the chat history, formulate a standalone question\\nwhich can be understood without the chat history. Do NOT answer the question,\\njust reformulate it if needed and otherwise return it as is.\\n'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7a916461eb60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7a916463b820>, root_client=<openai.OpenAI object at 0x7a9165299660>, root_async_client=<openai.AsyncOpenAI object at 0x7a916461e3b0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
              "| StrOutputParser()\n",
              "| VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7a914af45ed0>, search_type='similarity_score_threshold', search_kwargs={'k': 5, 'score_threshold': 0.2})), kwargs={}, config={'run_name': 'chat_retriever_chain'}, config_factories=[])"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history_aware_retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN2SIBOs29Si"
      },
      "source": [
        "This chain prepends a rephrasing of the input query to our retriever, so that the retrieval incorporates the context of the conversation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lm91Yl5eJg"
      },
      "source": [
        "## Building the QA RAG Chain with Chat History\n",
        "\n",
        "Now we're ready to construct our comprehensive QA RAG chain, which leverages historical context for more accurate and relevant responses.\n",
        "\n",
        "### Components of the QA RAG Chain\n",
        "\n",
        "1. **Creating Document Chains:**\n",
        "   - We use the `create_stuff_documents_chain` function, which is detailed [here](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html). This function is used to create a `question_answer_chain`, accepting inputs such as `context`, `chat_history`, and `input`. It efficiently combines the retrieved context with the conversation history and the current query to generate an informed answer.\n",
        "\n",
        "2. **Building the Final QA RAG Chain:**\n",
        "   - The entire QA RAG chain is assembled using the `create_retrieval_chain` function, available [here](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html). This chain integrates the `history_aware_retriever` with the `question_answer_chain`. It retains intermediate outputs like the retrieved context for added convenience during the query handling process.\n",
        "   - The `create_retrieval_chain` function accepts keys such as `input` and `chat_history` and includes `input`, `chat_history`, `context`, and `answer` in its outputs.\n",
        "\n",
        "By implementing these steps, the system not only contextualizes but also provides accurate answers by synthesizing information from both the current and historical interactions. This method enhances the conversational AI’s ability to understand and respond to user queries dynamically, making the interactions more engaging and relevant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8v6WgeG5KQa",
        "outputId": "c98ff992-c6e0-43f0-c9d7-d3766699d372"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
              "  context: RunnableLambda(format_docs)\n",
              "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
              "| ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7a9165aee290>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks.\\n                      Use the following pieces of retrieved context to answer the question.\\n                      Also refer to the whole historical conversation if you need additional\\n                      context to answer the question\\n                      If the answer is not present in the context and historical conversation,\\n                      just say that you don't know.\\n                      Keep the answer to the point.\\n\\n                      Context:\\n                      {context}\\n\\n                  \"), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='Question:\\n                     {input}\\n\\n                     Answer:\\n                  '), additional_kwargs={})])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7a916461eb60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7a916463b820>, root_client=<openai.OpenAI object at 0x7a9165299660>, root_async_client=<openai.AsyncOpenAI object at 0x7a916461e3b0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
              "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "                      Use the following pieces of retrieved context to answer the question.\n",
        "                      Also refer to the whole historical conversation if you need additional\n",
        "                      context to answer the question\n",
        "                      If the answer is not present in the context and historical conversation,\n",
        "                      just say that you don't know.\n",
        "                      Keep the answer to the point.\n",
        "\n",
        "                      Context:\n",
        "                      {context}\n",
        "\n",
        "                  \"\"\"\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"\"\"Question:\n",
        "                     {input}\n",
        "\n",
        "                     Answer:\n",
        "                  \"\"\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(chatgpt, qa_prompt)\n",
        "question_answer_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNhAPQRh-bIM"
      },
      "outputs": [],
      "source": [
        "qa_rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TDpC7QR-fd_",
        "outputId": "cb7a95ba-7d3a-4f53-dab0-99cd51285f46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RunnableBinding(bound=RunnableAssign(mapper={\n",
              "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
              "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7a914af45ed0>, search_type='similarity_score_threshold', search_kwargs={'k': 5, 'score_threshold': 0.2}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7a9165aee290>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Given a chat history and the latest user question\\nwhich might reference context in the chat history, formulate a standalone question\\nwhich can be understood without the chat history. Do NOT answer the question,\\njust reformulate it if needed and otherwise return it as is.\\n'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
              "           | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7a916461eb60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7a916463b820>, root_client=<openai.OpenAI object at 0x7a9165299660>, root_async_client=<openai.AsyncOpenAI object at 0x7a916461e3b0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
              "           | StrOutputParser()\n",
              "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7a914af45ed0>, search_type='similarity_score_threshold', search_kwargs={'k': 5, 'score_threshold': 0.2})), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
              "})\n",
              "| RunnableAssign(mapper={\n",
              "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
              "              context: RunnableLambda(format_docs)\n",
              "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
              "            | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7a9165aee290>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks.\\n                      Use the following pieces of retrieved context to answer the question.\\n                      Also refer to the whole historical conversation if you need additional\\n                      context to answer the question\\n                      If the answer is not present in the context and historical conversation,\\n                      just say that you don't know.\\n                      Keep the answer to the point.\\n\\n                      Context:\\n                      {context}\\n\\n                  \"), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='Question:\\n                     {input}\\n\\n                     Answer:\\n                  '), additional_kwargs={})])\n",
              "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7a916461eb60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7a916463b820>, root_client=<openai.OpenAI object at 0x7a9165299660>, root_async_client=<openai.AsyncOpenAI object at 0x7a916461e3b0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
              "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
              "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa_rag_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "mPguXvkG7b6c",
        "outputId": "aef963fc-7e00-4794-bba5-91bad884dbe4"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Machine learning is a subfield of computer science that gives computers the ability to learn without being explicitly programmed. It involves the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. Machine learning is used in situations where designing and programming explicit algorithms is not feasible, with applications including spam filtering, network intrusion detection, optical character recognition, search engines, and computer vision."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "chat_history = []\n",
        "\n",
        "question = \"What is machine learning?\"\n",
        "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
        "display(Markdown(response['answer']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZxkLV4bJkOt",
        "outputId": "9697791a-3e00-4406-f207-a765d8025ee0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_fPkZ9P7rF4",
        "outputId": "f9f08244-0470-4fc3-d68a-e0df28c1512c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='What is machine learning?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Machine learning is a subfield of computer science that gives computers the ability to learn without being explicitly programmed. It involves the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. Machine learning is used in situations where designing and programming explicit algorithms is not feasible, with applications including spam filtering, network intrusion detection, optical character recognition, search engines, and computer vision.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "chat_history.extend([HumanMessage(content=question),\n",
        "                     AIMessage(content=response[\"answer\"])])\n",
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "nZRM06Y375LF",
        "outputId": "14c8e8e9-c8c9-4183-d2ae-157b2d1da85b"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Machine learning and statistics differ primarily in their focus and approach. Statistics is concerned with data collection, organization, analysis, interpretation, and presentation, often emphasizing inference and making predictions based on sample data. It relies on established mathematical theories and models to understand data relationships.\n",
              "\n",
              "In contrast, machine learning focuses on building algorithms that can learn from data and improve their performance over time without being explicitly programmed. While both fields use data, machine learning often deals with larger datasets and aims to create models that can generalize well to new, unseen data. Additionally, machine learning emphasizes prediction and decision-making, whereas statistics often emphasizes understanding and interpreting data relationships."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "question = \"How is it different from statistics\"\n",
        "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
        "display(Markdown(response['answer']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozckE4Hr8yOj",
        "outputId": "b8ffb351-8dac-4e55-d227-1e73b9cc3966"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='What is machine learning?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Machine learning is a subfield of computer science that gives computers the ability to learn without being explicitly programmed. It involves the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. Machine learning is used in situations where designing and programming explicit algorithms is not feasible, with applications including spam filtering, network intrusion detection, optical character recognition, search engines, and computer vision.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='How is it different from statistics', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Machine learning and statistics differ primarily in their focus and approach. Statistics is concerned with data collection, organization, analysis, interpretation, and presentation, often emphasizing inference and making predictions based on sample data. It relies on established mathematical theories and models to understand data relationships.\\n\\nIn contrast, machine learning focuses on building algorithms that can learn from data and improve their performance over time without being explicitly programmed. While both fields use data, machine learning often deals with larger datasets and aims to create models that can generalize well to new, unseen data. Additionally, machine learning emphasizes prediction and decision-making, whereas statistics often emphasizes understanding and interpreting data relationships.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history.extend([HumanMessage(content=question),\n",
        "                     AIMessage(content=response[\"answer\"])])\n",
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "xQqNj73E8zVL",
        "outputId": "46242d09-8f61-4263-b972-9e15f9fc2292"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The chi-square test is a statistical test used to determine whether there is a significant association between categorical variables. It compares the observed frequencies in each category of a contingency table to the frequencies expected under the null hypothesis of no association. The test calculates a chi-square statistic, which measures how much the observed counts deviate from the expected counts. A high chi-square value indicates a significant difference between observed and expected frequencies, leading to the rejection of the null hypothesis. The chi-square test is commonly used in tests of independence and goodness-of-fit."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "question = \"What is the chi square test?\"\n",
        "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
        "display(Markdown(response['answer']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op5Lb-9uJ5aI",
        "outputId": "78920afb-0e26-4f74-bccc-12dcb9078796"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'What is the chi square test?',\n",
              " 'chat_history': [HumanMessage(content='What is machine learning?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Machine learning is a subfield of computer science that gives computers the ability to learn without being explicitly programmed. It involves the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. Machine learning is used in situations where designing and programming explicit algorithms is not feasible, with applications including spam filtering, network intrusion detection, optical character recognition, search engines, and computer vision.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='How is it different from statistics', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Machine learning and statistics differ primarily in their focus and approach. Statistics is concerned with data collection, organization, analysis, interpretation, and presentation, often emphasizing inference and making predictions based on sample data. It relies on established mathematical theories and models to understand data relationships.\\n\\nIn contrast, machine learning focuses on building algorithms that can learn from data and improve their performance over time without being explicitly programmed. While both fields use data, machine learning often deals with larger datasets and aims to create models that can generalize well to new, unseen data. Additionally, machine learning emphasizes prediction and decision-making, whereas statistics often emphasizes understanding and interpreting data relationships.', additional_kwargs={}, response_metadata={})],\n",
              " 'context': [Document(metadata={'id': '71548', 'title': 'Chi-square distribution'}, page_content='In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1\\xa0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution. Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.'),\n",
              "  Document(metadata={'id': '470230', 'title': 'Statistical significance'}, page_content='Statistics uses variables to describe a measurement. Such a variable is called statistically significant if under a certain status quo assumption, the probability of obtaining its outcome (or a more extreme outcome) is less than a given value. Statistical significance is hence a way of determining the \"unlikeliness\" of an experimental result—when a certain status quo assumption is assumed to be true. Statistical hypothesis tests are used to check significance. The concept of statistical significance was originated by Ronald Fisher in his 1925 publication, \"Statistical Methods for Research Workers\", when he developed statistical hypothesis testing (which he described as \"tests of significance\"). Fisher suggested a probability of one in twenty (0.05 or 5%)—as a convenient cutoff level to reject the null hypothesis. In their 1933 paper, Jerzy Neyman and Egon Pearson recommended that the significance level (for example 0.05), which they called α, be set before any data collection. Despite his initial suggestion of 0.05 as a significance level, Fisher did not intend this cutoff value to be fixed. In his 1956 publication \"Statistical methods and scientific inference\", he recommended that significant levels be set according to specific circumstances.'),\n",
              "  Document(metadata={'id': '754511', 'title': 'Kolmogorov–Smirnov test'}, page_content='The Kolmogorov–Smirnov test is a test from statistics. This test is done either to show that two random variables follow the same distribution, or that one random variable follows a given distribution. It is named after Andrey Kolmogorov and Nikolai Smirnov.'),\n",
              "  Document(metadata={'id': '66398', 'title': 'Frequency distribution'}, page_content='In statistics, a frequency distribution is a list of the values that a variable takes in a sample. It is usually a list, ordered by quantity. It will show the number of times each value appears. For example, if 100 people rate a five-point Likert scale assessing their agreement with a statement on a scale on which 1 denotes strong agreement and 5 strong disagreement, the frequency distribution of their responses might look like: This simple table has two drawbacks. When a variable can take continuous values instead of discrete values or when the number of possible values is too large, the table construction is difficult, if it is not impossible. A slightly different scheme based on the range of values is used in such cases. For example, if we consider the heights of the students in a class, the frequency table might look like below. Managing and operating on frequency tabulated data is much simpler than operation on raw data. There are simple algorithms to calculate median, mean (statistics), standard deviation etc. from these tables.'),\n",
              "  Document(metadata={'id': '531764', 'title': 'Rank correlation'}, page_content='A rank correlation is any statistic that measures the relationship between rankings. A \"ranking\" is the assignment of \"first\", \"second\", \"third\", etc. to different observations of a variable. A rank correlation coefficient measures the degree of similarity between two rankings. One might test for \"do colleges with a higher-ranked basketball program tend to have a higher-ranked football program?\" A quite important question is \"do people with higher-ranked education tend to get higher levels of income?\" Some of the most used rank correlation statistics are')],\n",
              " 'answer': 'The chi-square test is a statistical test used to determine whether there is a significant association between categorical variables. It compares the observed frequencies in each category of a contingency table to the frequencies expected under the null hypothesis of no association. The test calculates a chi-square statistic, which measures how much the observed counts deviate from the expected counts. A high chi-square value indicates a significant difference between observed and expected frequencies, leading to the rejection of the null hypothesis. The chi-square test is commonly used in tests of independence and goodness-of-fit.'}"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra8o20RH88WQ"
      },
      "outputs": [],
      "source": [
        "chat_history.extend([HumanMessage(content=question),\n",
        "                     AIMessage(content=response[\"answer\"])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNkuQx7DezF5",
        "outputId": "ffda2d3e-ead9-4624-ff1d-ff981f349ae8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='What is machine learning?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Machine learning is a subfield of computer science that gives computers the ability to learn without being explicitly programmed. It involves the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. Machine learning is used in situations where designing and programming explicit algorithms is not feasible, with applications including spam filtering, network intrusion detection, optical character recognition, search engines, and computer vision.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='How is it different from statistics', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Machine learning and statistics differ primarily in their focus and approach. Statistics is concerned with data collection, organization, analysis, interpretation, and presentation, often emphasizing inference and making predictions based on sample data. It relies on established mathematical theories and models to understand data relationships.\\n\\nIn contrast, machine learning focuses on building algorithms that can learn from data and improve their performance over time without being explicitly programmed. While both fields use data, machine learning often deals with larger datasets and aims to create models that can generalize well to new, unseen data. Additionally, machine learning emphasizes prediction and decision-making, whereas statistics often emphasizes understanding and interpreting data relationships.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What is the chi square test?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='The chi-square test is a statistical test used to determine whether there is a significant association between categorical variables. It compares the observed frequencies in each category of a contingency table to the frequencies expected under the null hypothesis of no association. The test calculates a chi-square statistic, which measures how much the observed counts deviate from the expected counts. A high chi-square value indicates a significant difference between observed and expected frequencies, leading to the rejection of the null hypothesis. The chi-square test is commonly used in tests of independence and goodness-of-fit.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84jh_iP5bEn5",
        "outputId": "e1a90469-3463-45ba-c875-fcaeae834a30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='How is it different from statistics', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Machine learning and statistics differ primarily in their focus and approach. Statistics is concerned with data collection, organization, analysis, interpretation, and presentation, often emphasizing inference and making predictions based on sample data. It relies on established mathematical theories and models to understand data relationships.\\n\\nIn contrast, machine learning focuses on building algorithms that can learn from data and improve their performance over time without being explicitly programmed. While both fields use data, machine learning often deals with larger datasets and aims to create models that can generalize well to new, unseen data. Additionally, machine learning emphasizes prediction and decision-making, whereas statistics often emphasizes understanding and interpreting data relationships.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What is the chi square test?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='The chi-square test is a statistical test used to determine whether there is a significant association between categorical variables. It compares the observed frequencies in each category of a contingency table to the frequencies expected under the null hypothesis of no association. The test calculates a chi-square statistic, which measures how much the observed counts deviate from the expected counts. A high chi-square value indicates a significant difference between observed and expected frequencies, leading to the rejection of the null hypothesis. The chi-square test is commonly used in tests of independence and goodness-of-fit.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history[-4:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s3Ii71fKa9U",
        "outputId": "8afa7ee7-7716-4613-a46d-bda0b9a9c763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The chi-square test and the t-test are different in several ways:\n",
            "\n",
            "1. **Type of Data**: The chi-square test is used for categorical data, while the t-test is used for continuous data.\n",
            "\n",
            "2. **Purpose**: The chi-square test assesses the association between categorical variables or tests the goodness-of-fit of observed data to a theoretical distribution. The t-test compares the means of two groups to determine if they are significantly different from each other.\n",
            "\n",
            "3. **Test Statistic**: The chi-square test uses the chi-square statistic, which is based on the difference between observed and expected frequencies. The t-test uses the t statistic, which is based on the difference between sample means relative to the variability of the samples.\n",
            "\n",
            "4. **Assumptions**: The chi-square test assumes that the observations are independent and that the expected frequency in each category is sufficiently large. The t-test assumes that the data is normally distributed and that the variances of the two groups are equal (for the independent t-test).\n",
            "\n",
            "Overall, the choice between the chi-square test and the t-test depends on the type of data and the research question being addressed.\n"
          ]
        }
      ],
      "source": [
        "question = \"How is it different from the t-test\"\n",
        "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history[-2:]})\n",
        "chat_history.extend([HumanMessage(content=question),\n",
        "                     AIMessage(content=response[\"answer\"])])\n",
        "display(Markdown(response['answer']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqdK6zX8RUk1",
        "outputId": "6fc3ccd4-e465-4e9a-bcf2-7b47ce54f3e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'How is it different from the t-test',\n",
              " 'chat_history': [HumanMessage(content='What is machine learning?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Machine learning is a subfield of computer science that gives computers the ability to learn without being explicitly programmed. It involves the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. Machine learning is used in situations where designing and programming explicit algorithms is not feasible, with applications including spam filtering, network intrusion detection, optical character recognition, search engines, and computer vision.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='How is it different from statistics', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Machine learning and statistics differ primarily in their focus and approach. Statistics is concerned with data collection, organization, analysis, interpretation, and presentation, often emphasizing inference and making predictions based on sample data. It relies on established mathematical theories and models to understand data relationships.\\n\\nIn contrast, machine learning focuses on building algorithms that can learn from data and improve their performance over time without being explicitly programmed. While statistics often seeks to explain relationships and make predictions based on assumptions about data distributions, machine learning emphasizes prediction accuracy and can handle larger and more complex datasets, often using techniques that may not have a clear statistical foundation.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is the chi square test?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The chi-square test is a statistical test used to determine whether there is a significant association between categorical variables. It compares the observed frequencies in each category of a contingency table to the frequencies expected under the null hypothesis of no association. The test calculates a chi-square statistic, which measures how much the observed counts deviate from the expected counts. If the chi-square statistic is large enough, it suggests that the observed data do not fit the expected distribution, leading to the rejection of the null hypothesis. The chi-square test is commonly used in tests of independence and goodness-of-fit.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='How is it different from the t-test', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The chi-square test and the t-test differ in their applications and the types of data they analyze. \\n\\n1. **Data Type**: The chi-square test is used for categorical data to assess relationships between variables, while the t-test is used for continuous data to compare the means of two groups.\\n\\n2. **Purpose**: The chi-square test evaluates whether there is a significant association between categorical variables (e.g., whether two groups differ in frequency counts), whereas the t-test assesses whether the means of two groups are statistically different from each other.\\n\\n3. **Assumptions**: The chi-square test assumes that the observations are independent and that the sample size is sufficiently large for the expected frequencies. The t-test assumes that the data are normally distributed and that the variances of the two groups are equal (in the case of the independent t-test).\\n\\nOverall, the choice between the chi-square test and the t-test depends on the nature of the data and the specific research question being addressed.', additional_kwargs={}, response_metadata={})],\n",
              " 'context': [Document(metadata={'id': '71548', 'title': 'Chi-square distribution'}, page_content='In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1\\xa0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution. Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.'),\n",
              "  Document(metadata={'id': '470230', 'title': 'Statistical significance'}, page_content='Statistics uses variables to describe a measurement. Such a variable is called statistically significant if under a certain status quo assumption, the probability of obtaining its outcome (or a more extreme outcome) is less than a given value. Statistical significance is hence a way of determining the \"unlikeliness\" of an experimental result—when a certain status quo assumption is assumed to be true. Statistical hypothesis tests are used to check significance. The concept of statistical significance was originated by Ronald Fisher in his 1925 publication, \"Statistical Methods for Research Workers\", when he developed statistical hypothesis testing (which he described as \"tests of significance\"). Fisher suggested a probability of one in twenty (0.05 or 5%)—as a convenient cutoff level to reject the null hypothesis. In their 1933 paper, Jerzy Neyman and Egon Pearson recommended that the significance level (for example 0.05), which they called α, be set before any data collection. Despite his initial suggestion of 0.05 as a significance level, Fisher did not intend this cutoff value to be fixed. In his 1956 publication \"Statistical methods and scientific inference\", he recommended that significant levels be set according to specific circumstances.'),\n",
              "  Document(metadata={'id': '66398', 'title': 'Frequency distribution'}, page_content='In statistics, a frequency distribution is a list of the values that a variable takes in a sample. It is usually a list, ordered by quantity. It will show the number of times each value appears. For example, if 100 people rate a five-point Likert scale assessing their agreement with a statement on a scale on which 1 denotes strong agreement and 5 strong disagreement, the frequency distribution of their responses might look like: This simple table has two drawbacks. When a variable can take continuous values instead of discrete values or when the number of possible values is too large, the table construction is difficult, if it is not impossible. A slightly different scheme based on the range of values is used in such cases. For example, if we consider the heights of the students in a class, the frequency table might look like below. Managing and operating on frequency tabulated data is much simpler than operation on raw data. There are simple algorithms to calculate median, mean (statistics), standard deviation etc. from these tables.'),\n",
              "  Document(metadata={'id': '310400', 'title': 'Grouped data'}, page_content=\"Grouped data is a statistical term used in data analysis. Raw data can be organized by grouping together similar measurements in a table. This frequency table is also called grouped data. For example, someone gave a group of students a simple math question, and timed how long it took them to answer it. The numbers are below: The smallest amount of time was 8 seconds, and the largest was 34 seconds. One method we could use to analyze the needed time is to group close numbers together. In order to keep the analysis fair, we'll make each group be the same number of seconds. We can then count how many students fell in each group. For example, if we organized scores into 5 second ranges:\"),\n",
              "  Document(metadata={'id': '208393', 'title': 'Descriptive statistics'}, page_content='Descriptive statistics is a branch of statistics. Its aim is to summarize a set of statistical data. The data are usually taken by sampling a population. To picture the way the data are distributed, a histogram may be drawn. The data may be summarized by computing some characterising values, like the \"center\" of the dat, and the \"spread\". In some cases the different items of data will be grouped and the groups will be described, in some way.')],\n",
              " 'answer': 'The chi-square test and the t-test differ in their applications and the types of data they analyze. \\n\\n1. **Data Type**: The chi-square test is used for categorical data to assess relationships between variables, while the t-test is used for continuous data to compare the means of two groups.\\n\\n2. **Purpose**: The chi-square test evaluates whether there is a significant association between categorical variables (e.g., whether two groups differ in frequency counts), whereas the t-test assesses whether the means of two groups are statistically different from each other.\\n\\n3. **Assumptions**: The chi-square test assumes that the observations are independent and that the sample size is sufficiently large for the expected frequencies. The t-test assumes that the data are normally distributed and that the variances of the two groups are equal (in the case of the independent t-test).\\n\\nOverall, the choice between the chi-square test and the t-test depends on the nature of the data and the specific research question being addressed.'}"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOFkt3K52SLt"
      },
      "source": [
        "# Multi-User Conversational RAG System with LangChain\n",
        "\n",
        "In many Q&A applications, the ability to engage in back-and-forth conversations with users is crucial. This necessitates the application having a form of \"memory\" to recall past interactions and apply this context to current queries.\n",
        "\n",
        "However in most real-world conversational systems, multiple users or user sessions will be accessing the system simultaneously.\n",
        "\n",
        "![](https://i.imgur.com/X4WivLu.gif)\n",
        "\n",
        "Here we will show how you can use `SQLChatMessageHistory` such that we can store separate conversation histories per user or session which is often the need for real-world chatbots which will be accessed by many users at the same time. Instead of in-memory we can store it in a SQL database which can be used to store a lot of conversations.\n",
        "\n",
        "We use a `get_session_history` function which is expected to take in a `session_id` and return a Message History object. Everything is stored in a SQL database. This `session_id` is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain\n",
        "\n",
        "We also use a `memory_buffer_window` function to only use the top-K last historical conversations before sending it to the LLM, basically our own implementation of `ConversationBufferWindowMemory`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OnEXZ113TEp",
        "outputId": "e4640c56-7a90-4a92-f8b1-c2139c686cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'memory.db': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# removes the memory database file - usually not needed\n",
        "# you can run this only when you want to remove all conversation histories\n",
        "!rm memory.db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sleeltMX3WfG"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "######### REPHRASER ############\n",
        "rephrase_system_prompt = \"\"\"Given a chat history and the latest user question\n",
        "which might reference context in the chat history, formulate a standalone question\n",
        "which can be understood without the chat history. Do NOT answer the question,\n",
        "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
        "\n",
        "rephrase_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", rephrase_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    chatgpt, similarity_retriever, rephrase_prompt\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "######### MULTI_USER RAG RESPONSE GENERATOR ############\n",
        "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "                      Use the following pieces of retrieved context to answer the question.\n",
        "                      Also refer to the whole historical conversation if you need additional\n",
        "                      context to answer the question\n",
        "                      If the answer is not present in the context and historical conversation,\n",
        "                      just say that you don't know.\n",
        "                      Keep the answer to the point.\n",
        "\n",
        "                      Context:\n",
        "                      {context}\n",
        "                  \"\"\"\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"\"\"Question:\n",
        "                     {input}\n",
        "\n",
        "                     Answer:\n",
        "                  \"\"\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# used to retrieve conversation history from database\n",
        "# based on a specific user or session ID\n",
        "def get_session_history_db(session_id):\n",
        "    history = SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")\n",
        "    return history\n",
        "\n",
        "# subset historical conversations based on last K conversation messages\n",
        "# here by default we use the last 10 conversations (ai-human) as memory to the input prompt\n",
        "def memory_buffer_window(messages, lastk_conversations=10):\n",
        "    return messages[-(lastk_conversations*2):] # each conversation has 2 messages - (human prompt, AI response)\n",
        "\n",
        "# custom RAG chain which looks at last K conversational messages\n",
        "question_answer_chain = (\n",
        "    RunnablePassthrough.assign(chat_history=lambda x: memory_buffer_window(x[\"chat_history\"]))\n",
        "      |\n",
        "    qa_prompt\n",
        "      |\n",
        "    chatgpt\n",
        "      |\n",
        "    StrOutputParser()\n",
        ")\n",
        "qa_rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "\n",
        "############ CONVERSATIONAL RAG CHAIN ####################\n",
        "conversational_rag_chain = RunnableWithMessageHistory(\n",
        "    qa_rag_chain,\n",
        "    get_session_history_db,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8Tx0vy3-bGp"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def conv_rag_chatbot(usersession_id, prompt):\n",
        "    response = conversational_rag_chain.invoke(\n",
        "                                {\"input\": prompt},\n",
        "                                config={\n",
        "                                    \"configurable\": {\"session_id\": usersession_id}\n",
        "                                }\n",
        "    )\n",
        "    print('Answer:')\n",
        "    display(Markdown(response['answer']))\n",
        "    print('Sources:')\n",
        "    for document in response['context']:\n",
        "        print(document)\n",
        "        print()\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "QaPLZp3T-7hb",
        "outputId": "09d09fdf-b3ab-42a7-d7a6-f60cb0f704bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/history.py:608: LangChainDeprecationWarning: `connection_string` was deprecated in LangChain 0.2.2 and will be removed in 1.0. Use connection instead.\n",
            "  message_history = self.get_session_history(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Machine learning is a subfield of computer science that gives computers the ability to learn without being explicitly programmed. It involves the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. Machine learning is particularly useful in situations where designing and programming explicit algorithms is not feasible. Examples of its applications include spam filtering, network intrusion detection, optical character recognition, search engines, and computer vision."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "page_content='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.' metadata={'id': '564928', 'title': 'Machine learning'}\n",
            "\n",
            "page_content='In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data.' metadata={'id': '359370', 'title': 'Supervised learning'}\n",
            "\n",
            "page_content='Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences.' metadata={'id': '663523', 'title': 'Deep learning'}\n",
            "\n",
            "page_content='Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology.' metadata={'id': '6360', 'title': 'Artificial intelligence'}\n",
            "\n",
            "page_content='A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation.' metadata={'id': '44742', 'title': 'Artificial neural network'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "us_id = 'bond007'\n",
        "r = conv_rag_chatbot(us_id, 'Explain what is machine learning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "XNwp9jKH74M2",
        "outputId": "3901c292-b989-4cdb-f8e2-7ccfe91173e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is a field of study that aims to make computers \"smart,\" enabling them to work independently without being explicitly programmed with commands. AI systems can interpret external data, learn from it, and use those learnings to achieve specific goals and tasks through flexible adaptation. In general, AI refers to programs that mimic human cognition, such as learning and problem-solving, although they do so in ways that differ from human thought processes."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "page_content='Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology.' metadata={'id': '6360', 'title': 'Artificial intelligence'}\n",
            "\n",
            "page_content='A.I. Artificial Intelligence, or A.I., is a 2001 American science fiction drama movie directed by Steven Spielberg. The screenplay was by Spielberg based on the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss. The movie was produced by Kathleen Kennedy, Spielberg and Bonnie Curtis. It stars Haley Joel Osment, Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt. It is set in a futuristic post-climate change society. \"A.I.\" tells the story of David (Osment), a childlike android uniquely programmed with the ability to love.' metadata={'id': '674015', 'title': 'A.I. Artificial Intelligence'}\n",
            "\n",
            "page_content='Artificial Intelligence is the tenth studio album by Welsh multi-instrumentalist John Cale. It was released in September 1985. It was his only full-length album on Beggars Banquet Records label. It was produced by Cale himself.' metadata={'id': '416249', 'title': 'Artificial Intelligence (album)'}\n",
            "\n",
            "page_content='Swarm Intelligence is a field of Computer science. It is a form of Artificial intelligence. Some animals, mostly insects like ants, or bees form large colonies. These colonies are made of many animals that communicate with each other. Each animal is relatively simple, but by working together with other animals it is able to solve complex tasks. Swarm intelligence wants to obtain similar behaviour than that observed with these animals. Instead of the animals, so called \"agents\" are used.' metadata={'id': '112634', 'title': 'Swarm intelligence'}\n",
            "\n",
            "page_content='A robot lawyer is an artificial intelligence (AI) computer program. It is designed to ask the same questions as a real lawyer about certain legal issues. Robot lawyers are being used in many countries around the world including the United States, the United Kingdom, and Holland.' metadata={'id': '564218', 'title': 'Robot lawyer'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "r = conv_rag_chatbot(us_id, 'Now explain what is AI')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "kZz9rmHs7b0h",
        "outputId": "8aca6ebd-6675-4ee7-de79-fb24eb71c2fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. The goal is to find the best-fitting straight line (linear predictor function) that can predict the value of the dependent variable based on the values of the independent variables. In linear regression, the coefficients of the independent variables are termed regression coefficients, and the model aims to minimize the difference between the observed values and the values predicted by the linear equation."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "page_content='In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable. Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis. In many of these models, the coefficients are referred to as \"weights\".' metadata={'id': '430252', 'title': 'Linear predictor function'}\n",
            "\n",
            "page_content='Logistic Regression, also known as \"Logit Regression\" or \"Logit Model\", is a mathematical model used in statistics to estimate (guess) the probability of an event occurring having been given some previous data. Logistic Regression works with binary data, where either the event happens (1) or the event does not happen (0). So given some feature x it tries to find out whether some event y happens or not. So y can either be 0 or 1. In the case where the event happens, y is given the value 1. If the event does not happen, then y is given the value of 0. For example, if y represents whether a sports team wins a match, then y will be 1 if they win the match or y will be 0 if they do not. This is known as \"Binomial Logistic Regression\". There is also another form of Logistic Regression which uses multiple values for the variable y. This form of Logistic Regression is known as \"Multinomial Logistic Regression\". Logistic Regression uses the logistic function to find a model that fits with the data points. The function gives an 'S' shaped curve to model the data. The curve is restricted between 0 and 1, so it is easy to apply when y is binary. Logistic Regression can then model events better than linear regression, as it shows the probability for y being 1 for a given x value. Logistic Regression is used in statistics and machine learning to predict values of an input from previous test data. Logistic regression is an alternative method to use other than the simpler Linear Regression. Linear regression tries to predict the data by finding a linear – straight line – equation to model or predict future data points. Logistic regression does not look at the relationship between the two variables as a straight line. Instead, Logistic regression uses the natural logarithm function to find the relationship between the variables and uses test data to find the coefficients. The function can then predict the future results using these coefficients in the logistic equation.' metadata={'id': '501529', 'title': 'Logistic Regression'}\n",
            "\n",
            "page_content='In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data.' metadata={'id': '359370', 'title': 'Supervised learning'}\n",
            "\n",
            "page_content='Statistical errors and residuals occur because measurement is never exact. It is not possible to do an exact measurement, but it is possible to say how accurate a measurement is. One can measure the same thing again and again, and collect all the data together. This allows us to do statistics on the data. What is meant by errors and residuals is the difference between the observed or measured value and the real value, which is unknown. If there is only one random variable, the difference between statistical errors and residuals is the difference between the mean of the population against the mean of the (observed) sample. In that case the residual is the difference between what the probability distribution says, and what was actually measured.' metadata={'id': '188232', 'title': 'Errors and residuals in statistics'}\n",
            "\n",
            "page_content='Backpropagation is a method of training neural networks to perform tasks more accurately. The algorithm was first used for this purpose in 1974 in papers published by Werbos, Rumelhart, Hinton, and Williams. The term backpropagation is short for \"backward propagation of errors\". It works especially well for feed forward neural networks (networks without any loops) and problems that require supervised learning. The idea is to test how wrong the neural network is and then correct it. This is repeated many times.' metadata={'id': '358084', 'title': 'Backpropagation'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "us_id = 'jim003'\n",
        "r = conv_rag_chatbot(us_id, 'What is linear regression?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "icEp-_b984-r",
        "outputId": "e8d99aad-fc58-4147-f127-4a49c69a5f53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Deep learning is a subset of machine learning that primarily uses neural networks with multiple layers (known as deep neural networks) to process data. It is particularly effective for tasks such as speech recognition, image recognition, and handwriting recognition, which are challenging for traditional algorithms. \n",
              "\n",
              "The key differences are:\n",
              "1. **Structure**: Deep learning models have multiple hidden layers that allow them to learn increasingly abstract representations of data, while traditional machine learning models typically have fewer layers.\n",
              "2. **Data Requirements**: Deep learning often requires large amounts of data to perform well, whereas traditional machine learning can work with smaller datasets.\n",
              "3. **Complexity**: Deep learning models are generally more complex and computationally intensive than traditional machine learning models.\n",
              "\n",
              "In summary, while all three—AI, machine learning, and deep learning—are interconnected, deep learning is a more specialized approach within machine learning that leverages complex neural networks to handle large-scale data and perform sophisticated tasks."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "page_content='Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences.' metadata={'id': '663523', 'title': 'Deep learning'}\n",
            "\n",
            "page_content='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.' metadata={'id': '564928', 'title': 'Machine learning'}\n",
            "\n",
            "page_content='Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology.' metadata={'id': '6360', 'title': 'Artificial intelligence'}\n",
            "\n",
            "page_content='A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation.' metadata={'id': '44742', 'title': 'Artificial neural network'}\n",
            "\n",
            "page_content='Reinforcement learning (RL) is teaching a \"software agent\" how to behave in an environment by telling it how good it's doing. It is an area of machine learning inspired by behaviorist psychology. Reinforcement learning is different from supervised learning because the correct inputs and outputs are never shown. Also, reinforcement learning usually learns as it goes (online learning) unlike supervised learning. This means an agent has to choose between exploring and sticking with what it knows best. A reinforcement learning system is made of a \"policy\" (formula_1), a \"reward function\" (formula_2), a \"value function\" (formula_3), and an optional \"model\" of the environment.' metadata={'id': '610032', 'title': 'Reinforcement learning'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "us_id = 'bond007'\n",
        "r = conv_rag_chatbot(us_id, 'what about deep learning and how is it different from the previous two')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "OroRf99D7rZF",
        "outputId": "6cc86461-357a-49b3-82ad-91236faf763e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Linear regression predicts continuous outcomes and assumes a linear relationship between the dependent and independent variables, resulting in a straight line equation. In contrast, logistic regression is used for binary outcomes (0 or 1) and models the probability of an event occurring using the logistic function, which produces an S-shaped curve. Logistic regression does not assume a linear relationship; instead, it estimates the probability of the dependent variable being 1 based on the independent variables."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources:\n",
            "page_content='Logistic Regression, also known as \"Logit Regression\" or \"Logit Model\", is a mathematical model used in statistics to estimate (guess) the probability of an event occurring having been given some previous data. Logistic Regression works with binary data, where either the event happens (1) or the event does not happen (0). So given some feature x it tries to find out whether some event y happens or not. So y can either be 0 or 1. In the case where the event happens, y is given the value 1. If the event does not happen, then y is given the value of 0. For example, if y represents whether a sports team wins a match, then y will be 1 if they win the match or y will be 0 if they do not. This is known as \"Binomial Logistic Regression\". There is also another form of Logistic Regression which uses multiple values for the variable y. This form of Logistic Regression is known as \"Multinomial Logistic Regression\". Logistic Regression uses the logistic function to find a model that fits with the data points. The function gives an 'S' shaped curve to model the data. The curve is restricted between 0 and 1, so it is easy to apply when y is binary. Logistic Regression can then model events better than linear regression, as it shows the probability for y being 1 for a given x value. Logistic Regression is used in statistics and machine learning to predict values of an input from previous test data. Logistic regression is an alternative method to use other than the simpler Linear Regression. Linear regression tries to predict the data by finding a linear – straight line – equation to model or predict future data points. Logistic regression does not look at the relationship between the two variables as a straight line. Instead, Logistic regression uses the natural logarithm function to find the relationship between the variables and uses test data to find the coefficients. The function can then predict the future results using these coefficients in the logistic equation.' metadata={'id': '501529', 'title': 'Logistic Regression'}\n",
            "\n",
            "page_content='In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable. Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis. In many of these models, the coefficients are referred to as \"weights\".' metadata={'id': '430252', 'title': 'Linear predictor function'}\n",
            "\n",
            "page_content='Binary neural network is an artificial neural network, where commonly used floating-point weights are replaced with binary ones. It largely saving the storage and computation, serves as a technique for deploying deep models on resource-limited devices. Usage of binary values can bring up to 58 times speedup, while accuracy and information capacity of binary neural network can be manually controlled. Binary neural networks do not achieve the same accuracy as their full-precision counterparts, but improvements are being made to close this gap.' metadata={'id': '779314', 'title': 'Binary Neural Network'}\n",
            "\n",
            "page_content='Statistical errors and residuals occur because measurement is never exact. It is not possible to do an exact measurement, but it is possible to say how accurate a measurement is. One can measure the same thing again and again, and collect all the data together. This allows us to do statistics on the data. What is meant by errors and residuals is the difference between the observed or measured value and the real value, which is unknown. If there is only one random variable, the difference between statistical errors and residuals is the difference between the mean of the population against the mean of the (observed) sample. In that case the residual is the difference between what the probability distribution says, and what was actually measured.' metadata={'id': '188232', 'title': 'Errors and residuals in statistics'}\n",
            "\n",
            "page_content='In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data.' metadata={'id': '359370', 'title': 'Supervised learning'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "us_id = 'jim003'\n",
        "r = conv_rag_chatbot(us_id, 'how is it different from logistic regression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ordvuCRiEHgm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
