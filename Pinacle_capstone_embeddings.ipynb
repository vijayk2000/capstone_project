{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# %pip install langchain_chroma\n",
        "# %pip install langchain_openai\n",
        "# %pip install langchain_google_genai\n",
        "# %pip install langchain_huggingface\n",
        "# %pip install langchain_community\n",
        "# %pip install FlagEmbedding\n",
        "# !pip install python-dotenv"
      ],
      "metadata": {
        "id": "ZaJV9KhOSR75"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ASdfdhag9G7S"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.docstore.document import Document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKPhiK_I9G7U"
      },
      "source": [
        "## Load the files and index them in a vector database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Mo3shsXf9G7U"
      },
      "outputs": [],
      "source": [
        "docs= []\n",
        "for doc in PyPDFDirectoryLoader(\"/content/data\").load():\n",
        "\n",
        "\n",
        "    metadata ={\n",
        "        \"source\": doc.metadata[\"source\"],\n",
        "        \"page\": doc.metadata[\"page\"],\n",
        "\n",
        "    }\n",
        "    doc_text = ' '.join(doc.page_content.split())\n",
        "    # Remove newlines and extra spaces\n",
        "    docs.append(Document(page_content=doc_text,\n",
        "                                   metadata=metadata))\n",
        "\n",
        "# PyPDFDirectoryLoader loads all PDFs in a directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCLh7yH49G7V",
        "outputId": "64fa80a5-d6de-41f4-8717-77eabfbfa0a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document(metadata={'source': '/content/data/mistral_paper.pdf', 'page': 1}, page_content='Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference implementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model. Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient. Through our work, our aim is to help the community create more affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications. 2 Architectural details Figure 1: Sliding Window Attention. The number of operations in vanilla attention is quadratic in the sequence length, and the memory increases linearly with the number of tokens. At inference time, this incurs higher latency and smaller throughput due to reduced cache availability. To alleviate this issue, we use sliding window attention: each token can attend to at most W tokens from the previous layer (here, W = 3). Note that tokens outside the sliding window still influence next word prediction. At each attention layer, information can move forward by W tokens. Hence, after k attention layers, information can move forward by up to k × W tokens. Parameter Value dim 4096 n_layers 32 head_dim 128 hidden_dim 14336 n_heads 32 n_kv_heads 8 window_size 4096 context_len 8192 vocab_size 32000 Table 1: Model architecture. Mistral 7B is based on a transformer architecture [ 27]. The main parameters of the architecture are summarized in Table 1. Compared to Llama, it introduces a few changes that we summarize below. Sliding Window Attention. SW A exploits the stacked layers of a trans- former to attend information beyond the window size W. The hidden state in position i of the layer k, hi, attends to all hidden states from the previous layer with positions between i − W and i. Recursively, hi can access tokens from the input layer at a distance of up to W × k tokens, as illustrated in Figure 1. At the last layer, using a window size of W = 4096, we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2')\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "pprint(docs[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "4C-NQ4_-9G7W"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "#from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from langchain_chroma import Chroma\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "# The splitting and chunking strategy\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "load_dotenv(\".env\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACE_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "3Ey9ms8t9G7X"
      },
      "outputs": [],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "rJXFLQef9G7X",
        "outputId": "0e22455e-6ce0-436c-d000-5142c090204d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Invalid device string: 'cuda0'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-e36d0a1108aa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'cuda0'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mencode_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'normalize_embeddings'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m hf = HuggingFaceEmbeddings(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_huggingface/embeddings/huggingface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             ) from exc\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         self._client = sentence_transformers.SentenceTransformer(\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_hpu_graph_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \"\"\"\n\u001b[0;32m-> 1302\u001b[0;31m         device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(\n\u001b[0m\u001b[1;32m   1303\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Invalid device string: 'cuda0'"
          ]
        }
      ],
      "source": [
        "# Download the embeddings\n",
        "open_ai_large = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "open_ai_small = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "1LtwzCMv9G7X"
      },
      "outputs": [],
      "source": [
        "embedding_list = [open_ai_large, open_ai_small, hf]\n",
        "embedding_names = [\"open_ai_large\", \"open_ai_small\", \"hf\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Sb_toguR9G7X"
      },
      "outputs": [],
      "source": [
        "chunked_docs = splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "6_AvClTo9G7Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# chroma_db = Chroma.from_documents(documents=chunked_docs,\n",
        "#                                   collection_name=f'capstone_db_bge',\n",
        "#                                   embedding=bge,\n",
        "#                                   collection_metadata={\"hnsw:space\": \"euclidean\"},\n",
        "#                                 #, \"bge\"  persist_directory=f\"./capstone_db_bge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8ca78929G7Y",
        "outputId": "e4916575-0f1b-4943-bd57-ec6839370e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using embedding: open_ai_large\n",
            "Chroma DB created for embedding: open_ai_large\n",
            "Using embedding: open_ai_small\n",
            "Chroma DB created for embedding: open_ai_small\n",
            "Using embedding: hf\n",
            "Chroma DB created for embedding: hf\n"
          ]
        }
      ],
      "source": [
        "for embedding, name in zip(embedding_list, embedding_names):\n",
        "    print(f\"Using embedding: {name}\")\n",
        "    # Create a Chroma vector store\n",
        "    chroma_db = Chroma.from_documents(documents=chunked_docs,\n",
        "                                      collection_name=f'capstone_db_{name}',\n",
        "                                      embedding=embedding,\n",
        "                                      collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                      persist_directory=f\"./capstone_db\")\n",
        "    print(f\"Chroma DB created for embedding: {name}\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "n5nJnLAM9G7Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# chroma_db = Chroma.from_documents(documents=chunked_docs,\n",
        "#                                   collection_name='capstone_db',\n",
        "#                                   embedding=openaiembedding,\n",
        "#                                   # need to set the distance function to cosine else it uses euclidean by default\n",
        "#                                   # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
        "#                                   collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "#                                   persist_directory=\"./capstone_db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug9IpSek9G7Z",
        "outputId": "fd214ea4-7850-44ba-fd32-9705a01cf99a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model name: ['open_ai_large']\n",
            "Retriever details: [[Document(id='e942dafe-4e1a-4565-bf76-d54146766e57', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='17c96125-bf85-4013-b3f5-517baf8ee654', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='b267b328-a103-4fe6-a4d5-4b37a4bcc165', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2'), Document(id='c19727d3-07ef-4933-ba41-5e9ade00b139', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2'), Document(id='e5a44a49-5611-4b7c-bb3e-f29d02f3b7bc', metadata={'page': 4, 'source': '/content/data/attention_paper.pdf'}, page_content='of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. • The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the')]]\n",
            "Model name: ['open_ai_large', 'open_ai_small']\n",
            "Retriever details: [[Document(id='e942dafe-4e1a-4565-bf76-d54146766e57', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='17c96125-bf85-4013-b3f5-517baf8ee654', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='b267b328-a103-4fe6-a4d5-4b37a4bcc165', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2'), Document(id='c19727d3-07ef-4933-ba41-5e9ade00b139', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2'), Document(id='e5a44a49-5611-4b7c-bb3e-f29d02f3b7bc', metadata={'page': 4, 'source': '/content/data/attention_paper.pdf'}, page_content='of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. • The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the')], [Document(id='91e251c7-070c-45bb-8108-334884dffb0d', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='7599876c-4885-4604-8373-8fed2aaf3ea8', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='c56d835a-857c-47a0-b49d-dc047fff7ec2', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='16d3be64-0504-4dce-a768-5c8e4f0a3a28', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='86d86e78-a104-4783-bcef-18ce8c5de7f2', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2')]]\n",
            "Model name: ['open_ai_large', 'open_ai_small', 'hf']\n",
            "Retriever details: [[Document(id='e942dafe-4e1a-4565-bf76-d54146766e57', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='17c96125-bf85-4013-b3f5-517baf8ee654', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='b267b328-a103-4fe6-a4d5-4b37a4bcc165', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2'), Document(id='c19727d3-07ef-4933-ba41-5e9ade00b139', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2'), Document(id='e5a44a49-5611-4b7c-bb3e-f29d02f3b7bc', metadata={'page': 4, 'source': '/content/data/attention_paper.pdf'}, page_content='of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. • The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the')], [Document(id='91e251c7-070c-45bb-8108-334884dffb0d', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='7599876c-4885-4604-8373-8fed2aaf3ea8', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='c56d835a-857c-47a0-b49d-dc047fff7ec2', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='16d3be64-0504-4dce-a768-5c8e4f0a3a28', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='86d86e78-a104-4783-bcef-18ce8c5de7f2', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2')], [Document(id='08129306-f8ab-4099-a221-90c7840897c8', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='482fba4a-a6ce-4daf-8fa0-7e88e40317a1', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='9ca15df2-b09c-40bb-a37c-c2844578398b', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='7a173475-c932-4fc9-8d8d-9cee0a478a4c', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='e0e907d0-96b4-4833-8838-b343b3df9703', metadata={'page': 13, 'source': '/content/data/attention_paper.pdf'}, page_content='Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word. 14')]]\n"
          ]
        }
      ],
      "source": [
        "# load from disk\n",
        "model_name =[]\n",
        "retriever_details = []\n",
        "combined_list = {}\n",
        "\n",
        "for embedding, name in zip(embedding_list, embedding_names):\n",
        "\n",
        "\n",
        "    chroma_db = Chroma(persist_directory=f\"./capstone_db\",\n",
        "\n",
        "                      collection_name=f'capstone_db_{name}',\n",
        "                       embedding_function=embedding)\n",
        "    similarity_retriever = chroma_db.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                              search_kwargs={\"k\": 5, \"score_threshold\": 0.0})\n",
        "    model_name.append(name)\n",
        "    print(f\"Model name: {model_name}\")\n",
        "    #print(embedding.model)\n",
        "    similarity = similarity_retriever.invoke(\"What is attention?\", k=5)\n",
        "    retriever_details.append(similarity)\n",
        "    print(f\"Retriever details: {retriever_details}\")\n",
        "    details = dict(zip(model_name, retriever_details))\n",
        "    combined_list.update(details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "zpZovpA39G7Z",
        "outputId": "dc440241-052e-45b5-b631-5e924ad05e05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "combined_list['open_ai_large'][0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvvM0nN89G7Z",
        "outputId": "8b3b75ea-32ee-4df1-fe99-560594cba40a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: open_ai_large\n",
            "Document: i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 2\n",
            "--------------------------------------------------------------------------------\n",
            "Document: i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 2\n",
            "--------------------------------------------------------------------------------\n",
            "Document: we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2\n",
            "Source: /content/data/mistral_paper.pdf\n",
            "Page: 1\n",
            "--------------------------------------------------------------------------------\n",
            "Document: we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2\n",
            "Source: /content/data/mistral_paper.pdf\n",
            "Page: 1\n",
            "--------------------------------------------------------------------------------\n",
            "Document: of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. • The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 4\n",
            "--------------------------------------------------------------------------------\n",
            "Model: open_ai_small\n",
            "Document: i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 2\n",
            "--------------------------------------------------------------------------------\n",
            "Document: i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 2\n",
            "--------------------------------------------------------------------------------\n",
            "Document: Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 12\n",
            "--------------------------------------------------------------------------------\n",
            "Document: Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 12\n",
            "--------------------------------------------------------------------------------\n",
            "Document: we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2\n",
            "Source: /content/data/mistral_paper.pdf\n",
            "Page: 1\n",
            "--------------------------------------------------------------------------------\n",
            "Model: hf\n",
            "Document: i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 2\n",
            "--------------------------------------------------------------------------------\n",
            "Document: i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 2\n",
            "--------------------------------------------------------------------------------\n",
            "Document: Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 12\n",
            "--------------------------------------------------------------------------------\n",
            "Document: Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 12\n",
            "--------------------------------------------------------------------------------\n",
            "Document: Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word. 14\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 13\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for model, retriever in combined_list.items():\n",
        "    print(f\"Model: {model}\")\n",
        "    for doc in retriever:\n",
        "        print(f\"Document: {doc.page_content}\")\n",
        "        #print(f\"Score: {doc.metadata['similarity_score']}\")\n",
        "        print(f\"Source: {doc.metadata['source']}\")\n",
        "        print(f\"Page: {doc.metadata['page']}\")\n",
        "        print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu8EcBRR9G7Z"
      },
      "source": [
        "### From the above embeddings, we can infer that text-embedding-3-large retrieves relevant data in the top."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### change to euclidean embeddings"
      ],
      "metadata": {
        "id": "NgGFXYwbw7vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for embedding, name in zip(embedding_list, embedding_names):\n",
        "    print(f\"Using embedding: {name}\")\n",
        "    # Create a Chroma vector store\n",
        "    chroma_db = Chroma.from_documents(documents=chunked_docs,\n",
        "                                      collection_name=f'capstone_db_{name}_euc',\n",
        "                                      embedding=embedding,\n",
        "\n",
        "                                      collection_metadata={\"hnsw:space\": \"l2\"},\n",
        "                                      persist_directory=f\"./capstone_db_euc\")\n",
        "    print(f\"Chroma DB created for embedding: {name}\")"
      ],
      "metadata": {
        "id": "PwPk2V7GrCTG",
        "outputId": "1a9005fd-3086-4441-cf27-b7ccac46a99a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using embedding: open_ai_large\n",
            "Chroma DB created for embedding: open_ai_large\n",
            "Using embedding: open_ai_small\n",
            "Chroma DB created for embedding: open_ai_small\n",
            "Using embedding: hf\n",
            "Chroma DB created for embedding: hf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4RG3EFJWzNJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDoY-bkn9G7Z",
        "outputId": "a2b7deba-9dee-4e9e-a722-89d821d42a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model name: ['open_ai_large', 'open_ai_small', 'hf']\n",
            "Retriever details: [[Document(id='e942dafe-4e1a-4565-bf76-d54146766e57', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='17c96125-bf85-4013-b3f5-517baf8ee654', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='b267b328-a103-4fe6-a4d5-4b37a4bcc165', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2'), Document(id='c19727d3-07ef-4933-ba41-5e9ade00b139', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2'), Document(id='e5a44a49-5611-4b7c-bb3e-f29d02f3b7bc', metadata={'page': 4, 'source': '/content/data/attention_paper.pdf'}, page_content='of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. • The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the')], [Document(id='91e251c7-070c-45bb-8108-334884dffb0d', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='7599876c-4885-4604-8373-8fed2aaf3ea8', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='c56d835a-857c-47a0-b49d-dc047fff7ec2', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='16d3be64-0504-4dce-a768-5c8e4f0a3a28', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='86d86e78-a104-4783-bcef-18ce8c5de7f2', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2')], [Document(id='08129306-f8ab-4099-a221-90c7840897c8', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='482fba4a-a6ce-4daf-8fa0-7e88e40317a1', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='9ca15df2-b09c-40bb-a37c-c2844578398b', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='7a173475-c932-4fc9-8d8d-9cee0a478a4c', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='e0e907d0-96b4-4833-8838-b343b3df9703', metadata={'page': 13, 'source': '/content/data/attention_paper.pdf'}, page_content='Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word. 14')]]\n",
            "Model name: ['open_ai_large', 'open_ai_small', 'hf']\n",
            "Retriever details: [[Document(id='e942dafe-4e1a-4565-bf76-d54146766e57', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='17c96125-bf85-4013-b3f5-517baf8ee654', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='b267b328-a103-4fe6-a4d5-4b37a4bcc165', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2'), Document(id='c19727d3-07ef-4933-ba41-5e9ade00b139', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2'), Document(id='e5a44a49-5611-4b7c-bb3e-f29d02f3b7bc', metadata={'page': 4, 'source': '/content/data/attention_paper.pdf'}, page_content='of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. • The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the')], [Document(id='91e251c7-070c-45bb-8108-334884dffb0d', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='7599876c-4885-4604-8373-8fed2aaf3ea8', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='c56d835a-857c-47a0-b49d-dc047fff7ec2', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='16d3be64-0504-4dce-a768-5c8e4f0a3a28', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='86d86e78-a104-4783-bcef-18ce8c5de7f2', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2')], [Document(id='08129306-f8ab-4099-a221-90c7840897c8', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='482fba4a-a6ce-4daf-8fa0-7e88e40317a1', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='9ca15df2-b09c-40bb-a37c-c2844578398b', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='7a173475-c932-4fc9-8d8d-9cee0a478a4c', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='e0e907d0-96b4-4833-8838-b343b3df9703', metadata={'page': 13, 'source': '/content/data/attention_paper.pdf'}, page_content='Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word. 14')]]\n",
            "Model name: ['open_ai_large', 'open_ai_small', 'hf']\n",
            "Retriever details: [[Document(id='e942dafe-4e1a-4565-bf76-d54146766e57', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='17c96125-bf85-4013-b3f5-517baf8ee654', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='b267b328-a103-4fe6-a4d5-4b37a4bcc165', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2'), Document(id='c19727d3-07ef-4933-ba41-5e9ade00b139', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2'), Document(id='e5a44a49-5611-4b7c-bb3e-f29d02f3b7bc', metadata={'page': 4, 'source': '/content/data/attention_paper.pdf'}, page_content='of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. • The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the')], [Document(id='91e251c7-070c-45bb-8108-334884dffb0d', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='7599876c-4885-4604-8373-8fed2aaf3ea8', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='c56d835a-857c-47a0-b49d-dc047fff7ec2', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='16d3be64-0504-4dce-a768-5c8e4f0a3a28', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='86d86e78-a104-4783-bcef-18ce8c5de7f2', metadata={'page': 1, 'source': '/content/data/mistral_paper.pdf'}, page_content='we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2')], [Document(id='08129306-f8ab-4099-a221-90c7840897c8', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='482fba4a-a6ce-4daf-8fa0-7e88e40317a1', metadata={'page': 2, 'source': '/content/data/attention_paper.pdf'}, page_content='i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3'), Document(id='9ca15df2-b09c-40bb-a37c-c2844578398b', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='7a173475-c932-4fc9-8d8d-9cee0a478a4c', metadata={'page': 12, 'source': '/content/data/attention_paper.pdf'}, page_content='Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13'), Document(id='e0e907d0-96b4-4833-8838-b343b3df9703', metadata={'page': 13, 'source': '/content/data/attention_paper.pdf'}, page_content='Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word. 14')]]\n"
          ]
        }
      ],
      "source": [
        "# load from disk\n",
        "model_name_euc =[]\n",
        "retriever_details_euc = []\n",
        "combined_list_euc = {}\n",
        "\n",
        "for embedding, name in zip(embedding_list, embedding_names):\n",
        "\n",
        "\n",
        "    chroma_db = Chroma(persist_directory=f\"./capstone_db_euc\",\n",
        "\n",
        "                      collection_name=f'capstone_db_{name}_euc',\n",
        "                       embedding_function=embedding)\n",
        "    similarity_retriever = chroma_db.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                              search_kwargs={\"k\": 5, \"score_threshold\": 0.0})\n",
        "    model_name_euc.append(name)\n",
        "    print(f\"Model name: {model_name}\")\n",
        "    #print(embedding.model)\n",
        "    similarity = similarity_retriever.invoke(\"What is attention?\", k=5)\n",
        "    retriever_details_euc.append(similarity)\n",
        "    print(f\"Retriever details: {retriever_details}\")\n",
        "    details = dict(zip(model_name, retriever_details))\n",
        "    combined_list_euc.update(details)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model, retriever in combined_list_euc.items():\n",
        "    print(f\"Model: {model}\")\n",
        "    for doc in retriever:\n",
        "        print(f\"Document: {doc.page_content}\")\n",
        "        #print(f\"Score: {doc.metadata['similarity_score']}\")\n",
        "        print(f\"Source: {doc.metadata['source']}\")\n",
        "        print(f\"Page: {doc.metadata['page']}\")\n",
        "        print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "hAaEE9u6zIpK",
        "outputId": "2b14c605-aaa6-48bd-c64e-bafc826ad9bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: open_ai_large\n",
            "Document: i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 2\n",
            "--------------------------------------------------------------------------------\n",
            "Document: i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 2\n",
            "--------------------------------------------------------------------------------\n",
            "Document: we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2\n",
            "Source: /content/data/mistral_paper.pdf\n",
            "Page: 1\n",
            "--------------------------------------------------------------------------------\n",
            "Document: we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2\n",
            "Source: /content/data/mistral_paper.pdf\n",
            "Page: 1\n",
            "--------------------------------------------------------------------------------\n",
            "Document: of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. • The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 4\n",
            "--------------------------------------------------------------------------------\n",
            "Model: open_ai_small\n",
            "Document: i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 2\n",
            "--------------------------------------------------------------------------------\n",
            "Document: i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 2\n",
            "--------------------------------------------------------------------------------\n",
            "Document: Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 12\n",
            "--------------------------------------------------------------------------------\n",
            "Document: Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 12\n",
            "--------------------------------------------------------------------------------\n",
            "Document: we have a theoretical attention span of approximately 131K tokens. In practice, for a sequence length of 16K andW = 4096, changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x speed improvement over a vanilla attention baseline. Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of W, and the keys and values for the timestep i are stored in position i mod W of the cache. As a result, when the position i is larger than W, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for W = 3. On a sequence length of 32k tokens, this reduces the cache memory usage by 8x, without impacting the model quality. 1https://github.com/mistralai/mistral-src 2https://github.com/skypilot-org/skypilot 3https://huggingface.co/mistralai 2\n",
            "Source: /content/data/mistral_paper.pdf\n",
            "Page: 1\n",
            "--------------------------------------------------------------------------------\n",
            "Model: hf\n",
            "Document: i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 2\n",
            "--------------------------------------------------------------------------------\n",
            "Document: i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 2\n",
            "--------------------------------------------------------------------------------\n",
            "Document: Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 12\n",
            "--------------------------------------------------------------------------------\n",
            "Document: Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 12\n",
            "--------------------------------------------------------------------------------\n",
            "Document: Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word. 14\n",
            "Source: /content/data/attention_paper.pdf\n",
            "Page: 13\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine similarity is better than Euclidean similarity.  We find the repetition of documents"
      ],
      "metadata": {
        "id": "HcNxgUQGzs3V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0owEJwE9G7a"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "            Use the following pieces of retrieved context to answer the question.\n",
        "            If the answer is not present in the context, just say that you don't know.\n",
        "            Keep the answer to the point. also show the top 3 context documents of the answer.\n",
        "\n",
        "            Question:\n",
        "            {question}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            Answer:\n",
        "         \"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQeLd0cJ9G7a"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Tyox5wI9G7a"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "qa_rag_chain = (\n",
        "    {\n",
        "        \"context\": (similarity_retriever\n",
        "                      |\n",
        "                    format_docs),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "       |\n",
        "    prompt_template\n",
        "      |\n",
        "    chatgpt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qspi829u9G7a",
        "outputId": "8c75faac-a0b3-49e2-d24e-60a94d62117e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Gemini is a family of highly capable multimodal models that includes evaluations at both the model and product levels, focusing on safety mitigations and user experience. It addresses critical policy areas such as hate speech and dangerous content, and adopts a user-centric approach to maximize diversity across various topics and user journeys.\n",
              "\n",
              "Top 3 context documents:\n",
              "1. Gemini: A Family of Highly Capable Multimodal Models 7.4.2. Gemini Advanced\n",
              "2. Gemini: A Family of Highly Capable Multimodal Models 7.4.2. Gemini Advanced\n",
              "3. Gemini: A Family of Highly Capable Multimodal Models 7.4.2. Gemini Advanced"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "query = \"What is gemini\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}